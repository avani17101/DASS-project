{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'usage': {u'text_characters': 43, u'features': 1, u'text_units': 1}, u'keywords': [{u'relevance': 0.90279, u'text': u'junk food', u'sentiment': {u'score': -0.779874}}, {u'relevance': 0.766517, u'text': u'Big business', u'sentiment': {u'score': -0.779874}}, {u'relevance': 0.52341, u'text': u'Brazil', u'sentiment': {u'score': -0.779874}}], u'language': u'en'}\n",
      "['junk food']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from watson_developer_cloud import NaturalLanguageUnderstandingV1\n",
    "import watson_developer_cloud.natural_language_understanding.features.v1 \\\n",
    "  as Features\n",
    "\n",
    "def watson(claim):\n",
    "    natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "      username=\"09b56387-57ee-4390-9365-a07a37706fb4\",\n",
    "      password=\"ISoTe5EueZJp\",\n",
    "      version=\"2017-02-27\")\n",
    "\n",
    "    response = natural_language_understanding.analyze(\n",
    "      text = claim,\n",
    "      features=[\n",
    "        Features.Keywords(\n",
    "          emotion=False,\n",
    "          sentiment=True,\n",
    "            limit=15\n",
    "        )\n",
    "      ]\n",
    "    )\n",
    "    keywords = []\n",
    "    for keyword in response['keywords']:\n",
    "        if keyword['relevance'] > 0.80 and len(keywords) < 8:\n",
    "            keywords.append(keyword['text'].encode('utf-8'))\n",
    "    print response\n",
    "    return keywords\n",
    "\n",
    "url = 'http://abcnews.go.com/Politics/wireStory/trump-supporters-critics-juggalos-descend-washington-49891059?cid=clicksource_4380645_1_hero_headlines_headlines_hed'\n",
    "claim = 'Big business got Brazil hooked on junk food'\n",
    "print watson(claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'usage': {u'text_characters': 43, u'features': 1, u'text_units': 1}, u'language': u'en', u'concepts': [{u'relevance': 0.885924, u'text': u'Nutrition', u'dbpedia_resource': u'http://dbpedia.org/resource/Nutrition'}, {u'relevance': 0.8188, u'text': u'Junk food', u'dbpedia_resource': u'http://dbpedia.org/resource/Junk_food'}]}\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from watson_developer_cloud import NaturalLanguageUnderstandingV1\n",
    "import watson_developer_cloud.natural_language_understanding.features.v1 \\\n",
    "  as Features\n",
    "\n",
    "def watson(claim):\n",
    "    natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "      username=\"09b56387-57ee-4390-9365-a07a37706fb4\",\n",
    "      password=\"ISoTe5EueZJp\",\n",
    "      version=\"2017-02-27\")\n",
    "\n",
    "    response = natural_language_understanding.analyze(\n",
    "      text = claim,\n",
    "      features=[\n",
    "        Features.Concepts(\n",
    "          # Concepts options\n",
    "          limit=3\n",
    "        )\n",
    "      ]\n",
    "    )\n",
    "    print response\n",
    "\n",
    "url = 'http://abcnews.go.com/Politics/wireStory/trump-supporters-critics-juggalos-descend-washington-49891059?cid=clicksource_4380645_1_hero_headlines_headlines_hed'\n",
    "claim = 'Big business got Brazil hooked on junk food'\n",
    "print watson(claim)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using user provided API key for making requests\n",
      "Event Registry host: http://eventregistry.org\n",
      "[['requests'], ['gold stocks', 'Watson Analytics'], ['Bob Dylan']]\n",
      "[['requests'], ['gold stocks', 'Watson Analytics'], ['Bob Dylan']]\n"
     ]
    }
   ],
   "source": [
    "from eventregistry import *\n",
    "from threading import Thread, Lock\n",
    "from watson_developer_cloud import NaturalLanguageUnderstandingV1\n",
    "from py_ms_cognitive import PyMsCognitiveWebSearch\n",
    "import watson_developer_cloud.natural_language_understanding.features.v1 as Features\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Print a list of recently added articles mentioning entered words\n",
    "api_key = 'eda39267-9017-481a-860d-0b565c6d8bf3'\n",
    "er = EventRegistry(apiKey = api_key)\n",
    "\n",
    "global_df = pd.DataFrame()\n",
    "mutex = Lock()\n",
    "claim = ''\n",
    "\n",
    "# Given keywords, this funciton appends the article metadata to the global pandas dataframe\n",
    "def get_articles(keywords):\n",
    "    global global_df\n",
    "    global claim\n",
    "    q = QueryArticlesIter(keywords=QueryItems.AND(keywords))\n",
    "    q.setRequestedResult(RequestArticlesInfo(count= 199, sortBy=\"sourceImportance\"))\n",
    "\n",
    "    x = 0\n",
    "\n",
    "    local_df = pd.DataFrame()\n",
    "\n",
    "    res = er.execQuery(q)\n",
    "    for article in res['articles']['results']:\n",
    "        if x is 0:\n",
    "            claim = article['title'].encode('utf-8'),\n",
    "        data = {\n",
    "            'source': article['source']['title'].encode('utf-8'),\n",
    "            'url' : article['url'].encode('utf-8'),\n",
    "            'text' : article['body'].encode('utf-8')\n",
    "        }\n",
    "\n",
    "        local_df = pd.concat([local_df, pd.DataFrame(data,index=[x])])\n",
    "        x += 1\n",
    "\n",
    "    mutex.acquire()\n",
    "    try:\n",
    "        global_df = pd.concat([global_df,local_df])\n",
    "    finally:\n",
    "        mutex.release()\n",
    "\n",
    "# Given a url, this function returns up to 15 keywords\n",
    "def watson(user_url):\n",
    "    natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "      username=\"09b56387-57ee-4390-9365-a07a37706fb4\",\n",
    "      password=\"ISoTe5EueZJp\",\n",
    "      version=\"2017-02-27\")\n",
    "\n",
    "    response = natural_language_understanding.analyze(\n",
    "      url=user_url,\n",
    "      features=[\n",
    "        Features.Keywords(\n",
    "          emotion=False,\n",
    "          sentiment=False,\n",
    "            limit=15\n",
    "        )\n",
    "      ]\n",
    "    )\n",
    "    keywords = []\n",
    "    for keyword in response['keywords']:\n",
    "        if keyword['relevance'] > 0.80 and len(keywords) < 8:\n",
    "            keywords.append(keyword['text'].encode('utf-8'))\n",
    "    return keywords\n",
    "\n",
    "# Worker thread class override\n",
    "class myThread(threading.Thread):\n",
    "    def __init__(self, query):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.query = query\n",
    "\n",
    "    def run(self):\n",
    "        get_articles(self.query)\n",
    "\n",
    "# given claim, azure returns related urls using bing searches\n",
    "def azure_search(claim):\n",
    "    search_term = claim\n",
    "    search_service = PyMsCognitiveWebSearch('75d1a40af4bf4ba4bdf561ae25b5db5c', claim)\n",
    "    first_three_result = search_service.search(limit=3, format='json') #1-50\n",
    "\n",
    "    urls = []\n",
    "   # To get individual result json:\n",
    "    for i in first_three_result:\n",
    "        urls.append(i.url.encode('utf-8'))\n",
    "    return urls\n",
    "\n",
    "# given a list of urls, this function returns all related keywords for the urls\n",
    "def azure_claim(urls):\n",
    "    keywords = []\n",
    "    for url in urls:\n",
    "        keywords.append(watson(url))\n",
    "    print keywords\n",
    "    return keywords\n",
    "\n",
    "# given keywords, query event registry and append to global dataframe\n",
    "def watson_azure_scrape(keywords):\n",
    "    global global_df\n",
    "\n",
    "    index = 0\n",
    "    threads = []\n",
    "\n",
    "    for query in keywords:\n",
    "        threads.append(myThread(query))\n",
    "        threads[index].start()\n",
    "        index += 1\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    global_df = global_df.reset_index(drop=True)\n",
    "    return global_df\n",
    "#     global_df.to_csv('watson_articles.csv')\n",
    "#     global_df['uid'] = range(len(global_df.index))\n",
    "#     return global_df.to_dict(orient='records')\n",
    "\n",
    "# Call this function with a claim to query event registry\n",
    "def run_azure(claim):\n",
    "    claim_tokens = nltk.word_tokenize(claim)\n",
    "    if len(claim_tokens) is 3:\n",
    "        # Go straight to event registry with claim\n",
    "        watson_azure_scrape(claim)\n",
    "    else:\n",
    "        watson_azure_scrape(azure_claim(azure_search(claim)))\n",
    "\n",
    "# Call this function with a url to query event registry\n",
    "def watson_scrape(url):\n",
    "    global global_df\n",
    "    keywords = watson(url)\n",
    "\n",
    "    index = 0\n",
    "    threads = []\n",
    "\n",
    "    for query in keywords:\n",
    "        threads.append(myThread(query))\n",
    "        threads[index].start()\n",
    "        index += 1\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    global_df = global_df.reset_index(drop=True)\n",
    "    return global_df\n",
    "    # global_df.to_csv('watson_articles.csv')\n",
    "#     global_df['uid'] = range(len(global_df.index))\n",
    "#     return global_df.to_dict(orient='records')\n",
    "\n",
    "url = 'http://abcnews.go.com/Politics/wireStory/trump-supporters-critics-juggalos-descend-washington-49891059?cid=clicksource_4380645_1_hero_headlines_headlines_hed'\n",
    "# print watson_scrape(url)\n",
    "\n",
    "claim = 'IBM watson is not very intelligent'\n",
    "# print run_azure(claim)\n",
    "\n",
    "print azure_claim(azure_search(claim))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import keras\n",
    "\n",
    "def load_kagglefakenews():\n",
    "  \n",
    "    df = pd.read_csv('Kaggle_FakeNews/train.csv', nrows=200, encoding='utf8')\n",
    "    train_data = df['text'].values.tolist() \n",
    "    train_labels = df['label'].values.tolist() \n",
    "\n",
    "\n",
    "    combo = list(zip(train_data, train_labels))\n",
    "    random.shuffle(combo)\n",
    "    train_data, train_labels = zip(*combo)\n",
    "    del df\n",
    "\n",
    "    return np.asarray(train_data).tolist(), np.asarray(train_labels).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels = load_kagglefakenews()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19554 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding\n",
    "from keras.utils import to_categorical\n",
    "import pickle\n",
    "\n",
    "MAX_NB_WORDS=50000 #dictionary size\n",
    "MAX_SEQUENCE_LENGTH=1500 #max word length of each individual article\n",
    "EMBEDDING_DIM=300 #dimensionality of the embedding vector (50, 100, 200, 300)\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n",
    "\n",
    "def tokenize_trainingdata(texts, labels):\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    pickle.dump(tokenizer, open('Models/tokenizer.p', 'wb'))\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    labels = to_categorical(labels, num_classes=len(set(labels)))\n",
    "\n",
    "    return data, labels, word_index\n",
    "\n",
    "#and run it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19554 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "X, Y, word_index = tokenize_trainingdata(train_data, train_labels)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data (90% train, 5% test, 5% validation)\n",
    "train_data = X[:int(len(X)*0.9)]\n",
    "train_labels = Y[:int(len(X)*0.9)]\n",
    "test_data = X[int(len(X)*0.9):int(len(X)*0.95)]\n",
    "test_labels = Y[int(len(X)*0.9):int(len(X)*0.95)]\n",
    "valid_data = X[int(len(X)*0.95):]\n",
    "valid_labels = Y[int(len(X)*0.95):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "def load_embeddings(word_index, embeddingsfile='wordEmbeddings/glove.6B.%id.txt' %EMBEDDING_DIM):\n",
    "    embeddings_index = {}\n",
    "    f = open(embeddingsfile, 'r', encoding='utf8')\n",
    "    for line in f:\n",
    "        #here we parse the data from the file\n",
    "        values = line.split(' ') #split the line by spaces\n",
    "        word = values[0] #each line starts with the word\n",
    "        coefs = np.asarray(values[1:], dtype='float32') #the rest of the line is the vector\n",
    "        embeddings_index[word] = coefs #put into embedding dictionary\n",
    "    f.close()\n",
    "\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)\n",
    "    return embedding_layer\n",
    "    \n",
    "#and build the embedding layer\n",
    "embedding_layer = load_embeddings(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential, Model, Input\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D, Flatten, Dense, GlobalAveragePooling1D, Dropout, LSTM, CuDNNLSTM, RNN, SimpleRNN, Conv2D, GlobalMaxPooling1D\n",
    "from keras import callbacks\n",
    "\n",
    "def baseline_model(sequence_input, embedded_sequences, classes=2):\n",
    "    x = Conv1D(64, 5, activation='relu')(embedded_sequences)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(128, 3, activation='relu')(x)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(256, 2, activation='relu')(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(2048, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    preds = Dense(classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/avani/anaconda3/lib/python3.7/site-packages/Keras-2.3.1-py3.7.egg/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1500)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1500, 300)         5866500   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1496, 64)          96064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 299, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 297, 128)          24704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 59, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 58, 256)           65792     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2048)              526336    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 7,629,510\n",
      "Trainable params: 1,763,010\n",
      "Non-trainable params: 5,866,500\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /home/avani/anaconda3/lib/python3.7/site-packages/Keras-2.3.1-py3.7.egg/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 180 samples, validate on 10 samples\n",
      "Epoch 1/25\n",
      "180/180 [==============================] - 3s 18ms/step - loss: 0.7990 - acc: 0.5000 - val_loss: 0.8890 - val_acc: 0.2000\n",
      "Epoch 2/25\n",
      "180/180 [==============================] - 2s 9ms/step - loss: 0.7099 - acc: 0.5111 - val_loss: 0.6277 - val_acc: 0.9000\n",
      "Epoch 3/25\n",
      "180/180 [==============================] - 2s 8ms/step - loss: 0.6775 - acc: 0.5889 - val_loss: 0.5580 - val_acc: 0.9000\n",
      "Epoch 4/25\n",
      "180/180 [==============================] - 2s 9ms/step - loss: 0.6741 - acc: 0.6056 - val_loss: 0.5593 - val_acc: 0.9000\n",
      "Epoch 5/25\n",
      "180/180 [==============================] - 2s 9ms/step - loss: 0.6734 - acc: 0.6444 - val_loss: 0.5846 - val_acc: 1.0000\n",
      "Epoch 6/25\n",
      "180/180 [==============================] - 2s 11ms/step - loss: 0.6537 - acc: 0.6278 - val_loss: 0.6090 - val_acc: 0.6000\n",
      "Epoch 7/25\n",
      "180/180 [==============================] - 2s 10ms/step - loss: 0.6350 - acc: 0.6722 - val_loss: 0.5702 - val_acc: 0.6000\n",
      "Epoch 8/25\n",
      "180/180 [==============================] - 2s 9ms/step - loss: 0.5993 - acc: 0.7222 - val_loss: 0.6091 - val_acc: 0.6000\n",
      "Epoch 9/25\n",
      "180/180 [==============================] - 2s 9ms/step - loss: 0.5499 - acc: 0.7444 - val_loss: 0.4995 - val_acc: 0.8000\n",
      "Epoch 10/25\n",
      "180/180 [==============================] - 2s 9ms/step - loss: 0.4636 - acc: 0.8278 - val_loss: 0.7914 - val_acc: 0.4000\n",
      "Epoch 11/25\n",
      "180/180 [==============================] - 2s 9ms/step - loss: 0.4539 - acc: 0.7889 - val_loss: 0.3987 - val_acc: 0.8000\n",
      "Epoch 12/25\n",
      "180/180 [==============================] - 2s 9ms/step - loss: 0.4156 - acc: 0.8444 - val_loss: 0.5621 - val_acc: 0.7000\n",
      "Epoch 13/25\n",
      "180/180 [==============================] - 2s 9ms/step - loss: 0.3523 - acc: 0.8111 - val_loss: 0.5316 - val_acc: 0.7000\n",
      "Epoch 14/25\n",
      "180/180 [==============================] - 2s 9ms/step - loss: 0.3061 - acc: 0.8722 - val_loss: 0.4864 - val_acc: 0.9000\n",
      "Epoch 15/25\n",
      "180/180 [==============================] - 2s 9ms/step - loss: 0.2559 - acc: 0.8833 - val_loss: 0.7475 - val_acc: 0.7000\n",
      "Epoch 16/25\n",
      "180/180 [==============================] - 2s 11ms/step - loss: 0.2132 - acc: 0.9333 - val_loss: 0.4904 - val_acc: 0.8000\n",
      "Epoch 17/25\n",
      "180/180 [==============================] - 2s 9ms/step - loss: 0.1482 - acc: 0.9556 - val_loss: 1.1465 - val_acc: 0.5000\n",
      "Epoch 18/25\n",
      "180/180 [==============================] - 2s 9ms/step - loss: 0.1205 - acc: 0.9611 - val_loss: 0.5281 - val_acc: 0.8000\n",
      "Epoch 19/25\n",
      "180/180 [==============================] - 2s 10ms/step - loss: 0.0893 - acc: 0.9889 - val_loss: 0.6520 - val_acc: 0.9000\n",
      "Epoch 20/25\n",
      "180/180 [==============================] - 2s 9ms/step - loss: 0.0608 - acc: 0.9944 - val_loss: 1.0670 - val_acc: 0.6000\n",
      "Epoch 21/25\n",
      "180/180 [==============================] - 2s 10ms/step - loss: 0.0461 - acc: 0.9889 - val_loss: 0.7633 - val_acc: 0.9000\n",
      "Epoch 22/25\n",
      "180/180 [==============================] - 2s 12ms/step - loss: 0.0347 - acc: 0.9944 - val_loss: 0.7635 - val_acc: 0.8000\n",
      "Epoch 23/25\n",
      "180/180 [==============================] - 2s 12ms/step - loss: 0.0301 - acc: 0.9944 - val_loss: 0.9268 - val_acc: 0.9000\n",
      "Epoch 24/25\n",
      "180/180 [==============================] - 2s 13ms/step - loss: 0.0233 - acc: 0.9944 - val_loss: 1.2753 - val_acc: 0.8000\n",
      "Epoch 25/25\n",
      "180/180 [==============================] - 2s 14ms/step - loss: 0.0206 - acc: 0.9944 - val_loss: 1.5797 - val_acc: 0.7000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fc5ebe33358>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#put embedding layer into input of the model\n",
    "MAX_SEQUENCE_LENGTH=1500\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "model = baseline_model(sequence_input, embedded_sequences, classes=2)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adamax',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(train_data, train_labels,\n",
    "          validation_data=(valid_data, valid_labels),\n",
    "          epochs=25, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "10/10 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5696607828140259, 0.800000011920929]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9888337  0.01116626]]\n"
     ]
    }
   ],
   "source": [
    "f1 = open('scraping/article.txt', \"r\")\n",
    "text = f1.read()\n",
    "  \n",
    "\n",
    "#tokenize\n",
    "tok = tokenize_text([text])\n",
    "\n",
    "\n",
    "print(model.predict(tok))  #%change of being real , fake\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
